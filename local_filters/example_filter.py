'''
This is an example filter which uses the cached files and file_stats.json to
speed up its execution. The filter runs through every file and replaces its
content with JSON data that contains the path to the file.
'''
# Timing the execution of the script
from time import perf_counter, sleep
TIME_START = perf_counter()

# The actual script
from pathlib import Path
import json
BP_PATH = Path("BP")
RP_PATH = Path("RP")


# Output postprocessing command here
POSTPROCESSING_PATH = Path("data/.cache/postprocessing")

# Log the actions of the filter here. Format: It's a dictionary:
# - 'deletions' - the list of deleted files
# - 'transformations' - a dictionary with pairs of
#   (source file path -> list of destination file paths). Note that the source
#   file path refers to a file before running the filter and the destination
#   file paths are the files after running the filter.
ACTIONS_LOG_PATH = Path("data/.cache/actions_log.json")

# Actions log from the previous run. It has some additional information. The
# paths are replaced with strings created with pattern: <file path>:<SHA>.
PREVIOUS_ACTIONS_PATH = Path("data/.cache/previous_actions.json")

# File generated by Regolith before running the filter. It's a dictionary with
# pairs of (file path -> SHA) loaded from RP and BP folders.
FILE_STATS_PATH = Path("data/.cache/file_stats.json")

if __name__ == "__main__":
    print(f"Example filter.")
    # Outputs (this is a part of the cache logic)
    postprocessing_output = []
    actions_log = {"deletions": [], "transformations": {}}
    # Inputs (this is a part of the cache logic)
    try:
        with FILE_STATS_PATH.open("r") as f:
            file_stats = json.load(f)
    except:
        file_stats = {}
    try:
        with PREVIOUS_ACTIONS_PATH.open("r") as f:
            previous_actions = json.load(f)
    except:
        previous_actions = {"deletions": [], "transformations": {}}

    # The actual code of the filter
    for root in [BP_PATH, RP_PATH]:
        for p in root.rglob("*"):
            if not p.is_file():
                continue
            # Check if the file is in the cache. This is a part of the cache
            # logic.
            if p.as_posix() in file_stats:
                p_as_posix = p.as_posix()
                action_key = f"{p_as_posix}:{file_stats[p_as_posix]}"
                print(action_key)
                if action_key in previous_actions["deletions"]:
                    postprocessing_output.append(f"delete {p_as_posix}")
                    actions_log["deletions"].append(p_as_posix)
                    continue
                if action_key in previous_actions["transformations"]:
                    dests = []
                    actions_log['transformations'][p_as_posix] = dests
                    for dest_path in previous_actions["transformations"][action_key]:
                        dest_path, dest_sha = dest_path.split(":")
                        dests.append(dest_path)
                        postprocessing_output.append(f"load {dest_path} {dest_sha}")
                    continue
            # File is not in the cache - process it
            sleep(0.1) # TOTALLY UNREALISTIC
            with p.open('w') as f:
                json.dump({"file_name": p.as_posix()}, f, indent='\t')
            # Save the action of the filter to the actions log
            actions_log["transformations"][p.as_posix()] = [p.as_posix()]
    # Outputs (this is a part of the cache logic)
    with POSTPROCESSING_PATH.open("w") as f:
        f.write("\n".join(postprocessing_output))
    with ACTIONS_LOG_PATH.open("w") as f:
        json.dump(actions_log, f, indent='\t')
    print(f"Example filter: Finished in {(perf_counter() - TIME_START)*100:.4f} ms")